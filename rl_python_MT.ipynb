{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_python_MT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrxc7zbuO6CeopI7qrmj7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhupee/MT5_TensorDL/blob/main/rl_python_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7poR1bn64i6U",
        "outputId": "7ac9a299-f386-45bf-8407-c6a8a54e78bc"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.8 is already the newest version (3.8.10-1+bionic1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hM9SNTx4BLj",
        "outputId": "86edd07e-8d26-44da-dca7-b079272dee1c"
      },
      "source": [
        "!pip install MetaTrader5==5.0.33"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement MetaTrader5==5.0.33 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for MetaTrader5==5.0.33\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "u49H4BR53g8y",
        "outputId": "dce1249f-6705-45c7-8859-c258a7b0f42a"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pylab as plt\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import MetaTrader5 as mt5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5e12c5853e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMetaTrader5\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmt5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MetaTrader5'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjvzF3xU31Ov"
      },
      "source": [
        "MAX_EPSILON = 1\n",
        "MIN_EPSILON = 0.01\n",
        "LAMBDA = 0.0001\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "nT284hcB2toW",
        "outputId": "cd9de226-b507-4306-d904-7732e69bf485"
      },
      "source": [
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, num_states, num_actions, batch_size):\n",
        "        self._num_states = num_states\n",
        "        self._num_actions = num_actions\n",
        "        self._batch_size = batch_size\n",
        "        # define the placeholders\n",
        "        self._states = None\n",
        "        self._actions = None\n",
        "        # the output operations\n",
        "        self._logits = None\n",
        "        self._optimizer = None\n",
        "        self._var_init = None\n",
        "        # now setup the model\n",
        "        self._define_model()\n",
        "\n",
        "    def _define_model(self):\n",
        "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
        "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
        "        # create a couple of fully connected hidden layers\n",
        "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
        "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
        "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
        "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
        "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        self._var_init = tf.global_variables_initializer()\n",
        "\n",
        "    def predict_one(self, state, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states:\n",
        "                                                     state.reshape(1, self.num_states)})\n",
        "\n",
        "    def predict_batch(self, states, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states: states})\n",
        "\n",
        "    def train_batch(self, sess, x_batch, y_batch):\n",
        "        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})\n",
        "\n",
        "    @property\n",
        "    def num_states(self):\n",
        "        return self._num_states\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return self._num_actions\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def var_init(self):\n",
        "        return self._var_init\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self, max_memory):\n",
        "        self._max_memory = max_memory\n",
        "        self._samples = []\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        self._samples.append(sample)\n",
        "        if len(self._samples) > self._max_memory:\n",
        "            self._samples.pop(0)\n",
        "\n",
        "    def sample(self, no_samples):\n",
        "        if no_samples > len(self._samples):\n",
        "            return random.sample(self._samples, len(self._samples))\n",
        "        else:\n",
        "            return random.sample(self._samples, no_samples)\n",
        "\n",
        "\n",
        "class GameRunner:\n",
        "    def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
        "                 decay, render=True):\n",
        "        self._sess = sess\n",
        "        self._env = env\n",
        "        self._model = model\n",
        "        self._memory = memory\n",
        "        self._render = render\n",
        "        self._max_eps = max_eps\n",
        "        self._min_eps = min_eps\n",
        "        self._decay = decay\n",
        "        self._eps = self._max_eps\n",
        "        self._steps = 0\n",
        "        self._reward_store = []\n",
        "        self._max_x_store = []\n",
        "\n",
        "    def run(self):\n",
        "        state = self._env.reset()\n",
        "        tot_reward = 0\n",
        "        max_x = -100\n",
        "        while True:\n",
        "            if self._render:\n",
        "                self._env.render()\n",
        "\n",
        "            action = self._choose_action(state)\n",
        "            next_state, reward, done, info = self._env.step(action)\n",
        "            if next_state[0] >= 0.1:\n",
        "                reward += 10\n",
        "            elif next_state[0] >= 0.25:\n",
        "                reward += 20\n",
        "            elif next_state[0] >= 0.5:\n",
        "                reward += 100\n",
        "\n",
        "            if next_state[0] > max_x:\n",
        "                max_x = next_state[0]\n",
        "            # is the game complete? If so, set the next state to\n",
        "            # None for storage sake\n",
        "            if done:\n",
        "                next_state = None\n",
        "\n",
        "            self._memory.add_sample((state, action, reward, next_state))\n",
        "            self._replay()\n",
        "\n",
        "            # exponentially decay the eps value\n",
        "            self._steps += 1\n",
        "            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \\\n",
        "                                      * math.exp(-LAMBDA * self._steps)\n",
        "\n",
        "            # move the agent to the next state and accumulate the reward\n",
        "            state = next_state\n",
        "            tot_reward += reward\n",
        "\n",
        "            # if the game is done, break the loop\n",
        "            if done:\n",
        "                self._reward_store.append(tot_reward)\n",
        "                self._max_x_store.append(max_x)\n",
        "                break\n",
        "\n",
        "        print(\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
        "\n",
        "    def _choose_action(self, state):\n",
        "        if random.random() < self._eps:\n",
        "            return random.randint(0, self._model.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(self._model.predict_one(state, self._sess))\n",
        "\n",
        "    def _replay(self):\n",
        "        batch = self._memory.sample(self._model.batch_size)\n",
        "        states = np.array([val[0] for val in batch])\n",
        "        next_states = np.array([(np.zeros(self._model.num_states)\n",
        "                                 if val[3] is None else val[3]) for val in batch])\n",
        "        # predict Q(s,a) given the batch of states\n",
        "        q_s_a = self._model.predict_batch(states, self._sess)\n",
        "        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
        "        q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
        "        # setup training arrays\n",
        "        x = np.zeros((len(batch), self._model.num_states))\n",
        "        y = np.zeros((len(batch), self._model.num_actions))\n",
        "        for i, b in enumerate(batch):\n",
        "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
        "            # get the current q values for all actions in state\n",
        "            current_q = q_s_a[i]\n",
        "            # update the q value for action\n",
        "            if next_state is None:\n",
        "                # in this case, the game completed after action, so there is no max Q(s',a')\n",
        "                # prediction possible\n",
        "                current_q[action] = reward\n",
        "            else:\n",
        "                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
        "            x[i] = state\n",
        "            y[i] = current_q\n",
        "        self._model.train_batch(self._sess, x, y)\n",
        "\n",
        "    @property\n",
        "    def reward_store(self):\n",
        "        return self._reward_store\n",
        "\n",
        "    @property\n",
        "    def max_x_store(self):\n",
        "        return self._max_x_store\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_name = 'MountainCar-v0'\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    num_states = env.env.observation_space.shape[0]\n",
        "    num_actions = env.env.action_space.n\n",
        "\n",
        "    model = Model(num_states, num_actions, BATCH_SIZE)\n",
        "    mem = Memory(50000)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.var_init)\n",
        "        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,\n",
        "                        LAMBDA)\n",
        "        num_episodes = 300\n",
        "        cnt = 0\n",
        "        while cnt < num_episodes:\n",
        "            if cnt % 10 == 0:\n",
        "                print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
        "            gr.run()\n",
        "            cnt += 1\n",
        "        plt.plot(gr.reward_store)\n",
        "        plt.show()\n",
        "        plt.close(\"all\")\n",
        "        plt.plot(gr.max_x_store)\n",
        "        plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9c8a0ad51b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9c8a0ad51b35>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_states, num_actions, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# now setup the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_define_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9c8a0ad51b35>\u001b[0m in \u001b[0;36m_define_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_define_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_s_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# create a couple of fully connected hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
          ]
        }
      ]
    }
  ]
}