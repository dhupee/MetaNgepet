{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_python_MT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrxc7zbuO6CeopI7qrmj7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python374jvsc74a57bd05272e271198e641207f4d8bec6a72159b91d0337ccfbc75f7129e4987471b9d3",
      "display_name": "Python 3.7.4 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4"
    },
    "metadata": {
      "interpreter": {
        "hash": "8b29cf3472b2de65ac0e4da98235d7879e936bb74c285ff9aa4315d2b599c5a6"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7poR1bn64i6U",
        "outputId": "7ac9a299-f386-45bf-8407-c6a8a54e78bc"
      },
      "source": [
        "#Python version\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hM9SNTx4BLj",
        "outputId": "86edd07e-8d26-44da-dca7-b079272dee1c"
      },
      "source": [
        "#Install uninstalled modules\n",
        "#!pip install ipykernel\n",
        "!pip install MetaTrader5==5.0.34\n",
        "!pip install gym\n",
        "!pip install gym-anytrading\n",
        "#!pip install tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MetaTrader5==5.0.34 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (5.0.34)\n",
            "Requirement already satisfied: numpy>=1.7 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from MetaTrader5==5.0.34) (1.19.2)\n",
            "Requirement already satisfied: gym in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
            "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym) (7.2.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
            "Requirement already satisfied: future in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
            "Requirement already satisfied: gym-anytrading in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (1.1.3)\n",
            "Requirement already satisfied: gym>=0.12.5 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (0.18.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (1.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->gym-anytrading) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->gym-anytrading) (2020.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym>=0.12.5->gym-anytrading) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym>=0.12.5->gym-anytrading) (1.6.0)\n",
            "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym>=0.12.5->gym-anytrading) (7.2.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym>=0.12.5->gym-anytrading) (1.5.2)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (2020.6.20)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->gym-anytrading) (1.15.0)\n",
            "Requirement already satisfied: future in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.12.5->gym-anytrading) (0.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "u49H4BR53g8y",
        "outputId": "dce1249f-6705-45c7-8859-c258a7b0f42a"
      },
      "source": [
        "#Import needed modules\n",
        "import gym\n",
        "import gym_anytrading\n",
        "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import datetime\n",
        "import MetaTrader5 as mt5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-52-3c4810beb818>, line 17)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-52-3c4810beb818>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    account = #Account number\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# display data on the MetaTrader 5 package\n",
        "print(\"MetaTrader5 package author: \",mt5.__author__)\n",
        "print(\"MetaTrader5 package version: \",mt5.__version__)\n",
        "\n",
        "# establish connection to the MetaTrader 5 terminal\n",
        "if not mt5.initialize():\n",
        "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
        "    mt5.shutdown()\n",
        " \n",
        "# display data on MetaTrader 5 version\n",
        "print(mt5.version())\n",
        "\n",
        "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
        "\n",
        "# Initialize Metatrader on a certain day\n",
        "'''if datetime.datetime.now() == weekdays:\n",
        "    # now connect to another trading account specifying the password\n",
        "    account = #Account number\n",
        "    password = #Password number\n",
        "    server = #Server name\n",
        "    authorized=mt5.login(account, password, server)\n",
        "    if authorized:\n",
        "        # display trading account data 'as is'\n",
        "        print(mt5.account_info())\n",
        "        # display trading account data in the form of a list\n",
        "        print(\"Show account_info()._asdict():\")\n",
        "        account_info_dict = mt5.account_info()._asdict()\n",
        "        for prop in account_info_dict:\n",
        "            print(\"  {}={}\".format(prop, account_info_dict[prop]))\n",
        "    else:\n",
        "        print(\"failed to connect at account #{}, error code: {}\".format(account, mt5.last_error()))\n",
        "# shut down connection to the MetaTrader 5 terminal\n",
        "else:\n",
        "    mt5.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "custom_env information:\n",
            "> shape: (10, 2)\n",
            "> df.shape: (6225, 5)\n",
            "> prices.shape: (300,)\n",
            "> signal_features.shape: (300, 2)\n",
            "> max_possible_profit: 1.122900180008982\n"
          ]
        }
      ],
      "source": [
        "custom_env = gym.make('forex-v0',\n",
        "               df = FOREX_EURUSD_1H_ASK,\n",
        "               window_size = 10,\n",
        "               frame_bound = (10, 300),\n",
        "               unit_side = 'right')\n",
        "\n",
        "print()\n",
        "print(\"custom_env information:\")\n",
        "print(\"> shape:\", custom_env.shape)\n",
        "print(\"> df.shape:\", custom_env.df.shape)\n",
        "print(\"> prices.shape:\", custom_env.prices.shape)\n",
        "print(\"> signal_features.shape:\", custom_env.signal_features.shape)\n",
        "print(\"> max_possible_profit:\", custom_env.max_possible_profit())\n",
        "\n",
        "#custom_env.reset() ##Uncomment if you need it.\n",
        "#custom_env.render()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjvzF3xU31Ov"
      },
      "source": [
        "EPS_NUMBER = 300\n",
        "MAX_EPSILON = 1\n",
        "MIN_EPSILON = 0.01\n",
        "LAMBDA = 0.0001\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 50"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model: #Model class\n",
        "    def __init__(self, num_states, num_actions, batch_size):\n",
        "        self._num_states = num_states\n",
        "        self._num_actions = num_actions\n",
        "        self._batch_size = batch_size\n",
        "        # define the placeholders\n",
        "        self._states = None\n",
        "        self._actions = None\n",
        "        # the output operations\n",
        "        self._logits = None\n",
        "        self._optimizer = None\n",
        "        self._var_init = None\n",
        "        # now setup the model\n",
        "        self._define_model()\n",
        "\n",
        "    def _define_model(self):\n",
        "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
        "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
        "        # create a couple of fully connected hidden layers\n",
        "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
        "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
        "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
        "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
        "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        self._var_init = tf.global_variables_initializer()\n",
        "\n",
        "    def predict_one(self, state, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states:\n",
        "                                                     state.reshape(1, self.num_states)})\n",
        "\n",
        "    def predict_batch(self, states, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states: states})\n",
        "\n",
        "    def train_batch(self, sess, x_batch, y_batch):\n",
        "        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})\n",
        "\n",
        "    @property\n",
        "    def num_states(self):\n",
        "        return self._num_states\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return self._num_actions\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def var_init(self):\n",
        "        return self._var_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_memory):\n",
        "        self._max_memory = max_memory\n",
        "        self._samples = []\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        self._samples.append(sample)\n",
        "        if len(self._samples) > self._max_memory:\n",
        "            self._samples.pop(0)\n",
        "\n",
        "    def sample(self, no_samples):\n",
        "        if no_samples > len(self._samples):\n",
        "            return random.sample(self._samples, len(self._samples))\n",
        "        else:\n",
        "            return random.sample(self._samples, no_samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "nT284hcB2toW",
        "outputId": "cd9de226-b507-4306-d904-7732e69bf485"
      },
      "source": [
        "class GameRunner:\n",
        "    def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
        "                 decay, render=True):\n",
        "        self._sess = sess\n",
        "        self._env = env\n",
        "        self._model = model\n",
        "        self._memory = memory\n",
        "        self._render = render\n",
        "        self._max_eps = max_eps\n",
        "        self._min_eps = min_eps\n",
        "        self._decay = decay\n",
        "        self._eps = self._max_eps\n",
        "        self._steps = 0\n",
        "        self._reward_store = []\n",
        "        self._max_x_store = []\n",
        "\n",
        "    def run(self):\n",
        "        state = self._env.reset()\n",
        "        tot_reward = 0\n",
        "        max_x = -100\n",
        "        while True:\n",
        "            if self._render:\n",
        "                self._env.render()\n",
        "\n",
        "            action = self._choose_action(state)\n",
        "            next_state, reward, done, info = self._env.step(action)\n",
        "            if next_state[0] >= 0.1:\n",
        "                reward += 10\n",
        "            elif next_state[0] >= 0.25:\n",
        "                reward += 20\n",
        "            elif next_state[0] >= 0.5:\n",
        "                reward += 100\n",
        "\n",
        "            if next_state[0] > max_x:\n",
        "                max_x = next_state[0]\n",
        "            # is the game complete? If so, set the next state to\n",
        "            # None for storage sake\n",
        "            if done:\n",
        "                next_state = None\n",
        "\n",
        "            self._memory.add_sample((state, action, reward, next_state))\n",
        "            self._replay()\n",
        "\n",
        "            # exponentially decay the eps value\n",
        "            self._steps += 1\n",
        "            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \\\n",
        "                                      * math.exp(-LAMBDA * self._steps)\n",
        "\n",
        "            # move the agent to the next state and accumulate the reward\n",
        "            state = next_state\n",
        "            tot_reward += reward\n",
        "\n",
        "            # if the game is done, break the loop\n",
        "            if done:\n",
        "                self._reward_store.append(tot_reward)\n",
        "                self._max_x_store.append(max_x)\n",
        "                break\n",
        "\n",
        "        print(\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
        "\n",
        "    def _choose_action(self, state):\n",
        "        if random.random() < self._eps:\n",
        "            return random.randint(0, self._model.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(self._model.predict_one(state, self._sess))\n",
        "\n",
        "    def _replay(self):\n",
        "        batch = self._memory.sample(self._model.batch_size)\n",
        "        states = np.array([val[0] for val in batch])\n",
        "        next_states = np.array([(np.zeros(self._model.num_states)\n",
        "                                 if val[3] is None else val[3]) for val in batch])\n",
        "        # predict Q(s,a) given the batch of states\n",
        "        q_s_a = self._model.predict_batch(states, self._sess)\n",
        "        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
        "        q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
        "        # setup training arrays\n",
        "        x = np.zeros((len(batch), self._model.num_states))\n",
        "        y = np.zeros((len(batch), self._model.num_actions))\n",
        "        for i, b in enumerate(batch):\n",
        "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
        "            # get the current q values for all actions in state\n",
        "            current_q = q_s_a[i]\n",
        "            # update the q value for action\n",
        "            if next_state is None:\n",
        "                # in this case, the game completed after action, so there is no max Q(s',a')\n",
        "                # prediction possible\n",
        "                current_q[action] = reward\n",
        "            else:\n",
        "                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
        "            x[i] = state\n",
        "            y[i] = current_q\n",
        "        self._model.train_batch(self._sess, x, y)\n",
        "\n",
        "    @property\n",
        "    def reward_store(self):\n",
        "        return self._reward_store\n",
        "\n",
        "    @property\n",
        "    def max_x_store(self):\n",
        "        return self._max_x_store"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'placeholder'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-ac9c5eb30894>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mnum_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-7-aa9f5d9c5953>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_states, num_actions, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_var_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# now setup the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_define_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-7-aa9f5d9c5953>\u001b[0m in \u001b[0;36m_define_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_define_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_q_s_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_actions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# create a couple of fully connected hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env_name = 'MountainCar-v0'\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    num_states = env.env.observation_space.shape[0]\n",
        "    num_actions = env.env.action_space.n\n",
        "\n",
        "    model = Model(num_states, num_actions, BATCH_SIZE)\n",
        "    mem = Memory(50000)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.var_init)\n",
        "        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,\n",
        "                        LAMBDA)\n",
        "        num_episodes = EPS_NUMBER\n",
        "        cnt = 0\n",
        "        while cnt < num_episodes:\n",
        "            if cnt % 10 == 0:\n",
        "                print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
        "            gr.run()\n",
        "            cnt += 1\n",
        "        plt.plot(gr.reward_store)\n",
        "        plt.show()\n",
        "        plt.close(\"all\")\n",
        "        plt.plot(gr.max_x_store)\n",
        "        plt.show()"
      ]
    }
  ]
}