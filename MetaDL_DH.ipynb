{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_python_MT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrxc7zbuO6CeopI7qrmj7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python374jvsc74a57bd05272e271198e641207f4d8bec6a72159b91d0337ccfbc75f7129e4987471b9d3",
      "display_name": "Python 3.7.4 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7poR1bn64i6U",
        "outputId": "7ac9a299-f386-45bf-8407-c6a8a54e78bc"
      },
      "source": [
        "#Python version\n",
        "!python --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hM9SNTx4BLj",
        "outputId": "86edd07e-8d26-44da-dca7-b079272dee1c"
      },
      "source": [
        "#Install uninstalled modules\n",
        "!pip install MetaTrader5==5.0.34\n",
        "!pip install gym-anytrading"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MetaTrader5==5.0.34 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (5.0.34)\n",
            "Requirement already satisfied: numpy>=1.7 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from MetaTrader5==5.0.34) (1.16.5)\n",
            "Requirement already satisfied: gym-anytrading in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (3.2.2)\n",
            "Requirement already satisfied: gym>=0.12.5 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (1.16.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from gym-anytrading) (0.25.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (2.8.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->gym-anytrading) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->gym-anytrading) (2019.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.1->gym-anytrading) (1.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dhupee\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->gym-anytrading) (41.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "u49H4BR53g8y",
        "outputId": "dce1249f-6705-45c7-8859-c258a7b0f42a"
      },
      "source": [
        "#Import needed modules\n",
        "import gym\n",
        "import gym_anytrading\n",
        "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import datetime\n",
        "import MetaTrader5 as mt5"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ncustom_env information:\n> shape: (10, 2)\n> df.shape: (6225, 5)\n> prices.shape: (300,)\n> signal_features.shape: (300, 2)\n> max_possible_profit: 1.122900180008982\n"
          ]
        }
      ],
      "source": [
        "custom_env = gym.make('forex-v0',\n",
        "               df = FOREX_EURUSD_1H_ASK,\n",
        "               window_size = 10,\n",
        "               frame_bound = (10, 300),\n",
        "               unit_side = 'right')\n",
        "\n",
        "print()\n",
        "print(\"custom_env information:\")\n",
        "print(\"> shape:\", custom_env.shape)\n",
        "print(\"> df.shape:\", custom_env.df.shape)\n",
        "print(\"> prices.shape:\", custom_env.prices.shape)\n",
        "print(\"> signal_features.shape:\", custom_env.signal_features.shape)\n",
        "print(\"> max_possible_profit:\", custom_env.max_possible_profit())\n",
        "\n",
        "#custom_env.reset() ##Uncomment if you need it.\n",
        "#custom_env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaTrader5 package author:  MetaQuotes Software Corp.\nMetaTrader5 package version:  5.0.34\ninitialize() failed, error code = (-10003, 'IPC initialize failed, MetaTrader 5 x64 not found')\n(0, 0, '')\n"
          ]
        }
      ],
      "source": [
        "# display data on the MetaTrader 5 package\n",
        "print(\"MetaTrader5 package author: \",mt5.__author__)\n",
        "print(\"MetaTrader5 package version: \",mt5.__version__)\n",
        " \n",
        "# establish connection to the MetaTrader 5 terminal\n",
        "if not mt5.initialize():\n",
        "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
        "    quit()\n",
        " \n",
        "# display data on MetaTrader 5 version\n",
        "print(mt5.version())\n",
        "\n",
        "'''\n",
        "# now connect to another trading account specifying the password\n",
        "account = #Account number\n",
        "password = #Password number\n",
        "server = #Server name\n",
        "authorized=mt5.login(account, password, server)\n",
        "if authorized:\n",
        "    # display trading account data 'as is'\n",
        "    print(mt5.account_info())\n",
        "    # display trading account data in the form of a list\n",
        "    print(\"Show account_info()._asdict():\")\n",
        "    account_info_dict = mt5.account_info()._asdict()\n",
        "    for prop in account_info_dict:\n",
        "        print(\"  {}={}\".format(prop, account_info_dict[prop]))\n",
        "else:\n",
        "    print(\"failed to connect at account #{}, error code: {}\".format(account, mt5.last_error()))\n",
        "''' \n",
        "# shut down connection to the MetaTrader 5 terminal\n",
        "mt5.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjvzF3xU31Ov"
      },
      "source": [
        "MAX_EPSILON = 1\n",
        "MIN_EPSILON = 0.01\n",
        "LAMBDA = 0.0001\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 50"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, num_states, num_actions, batch_size):\n",
        "        self._num_states = num_states\n",
        "        self._num_actions = num_actions\n",
        "        self._batch_size = batch_size\n",
        "        # define the placeholders\n",
        "        self._states = None\n",
        "        self._actions = None\n",
        "        # the output operations\n",
        "        self._logits = None\n",
        "        self._optimizer = None\n",
        "        self._var_init = None\n",
        "        # now setup the model\n",
        "        self._define_model()\n",
        "\n",
        "    def _define_model(self):\n",
        "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
        "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
        "        # create a couple of fully connected hidden layers\n",
        "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
        "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
        "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
        "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
        "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        self._var_init = tf.global_variables_initializer()\n",
        "\n",
        "    def predict_one(self, state, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states:\n",
        "                                                     state.reshape(1, self.num_states)})\n",
        "\n",
        "    def predict_batch(self, states, sess):\n",
        "        return sess.run(self._logits, feed_dict={self._states: states})\n",
        "\n",
        "    def train_batch(self, sess, x_batch, y_batch):\n",
        "        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})\n",
        "\n",
        "    @property\n",
        "    def num_states(self):\n",
        "        return self._num_states\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return self._num_actions\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def var_init(self):\n",
        "        return self._var_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_memory):\n",
        "        self._max_memory = max_memory\n",
        "        self._samples = []\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        self._samples.append(sample)\n",
        "        if len(self._samples) > self._max_memory:\n",
        "            self._samples.pop(0)\n",
        "\n",
        "    def sample(self, no_samples):\n",
        "        if no_samples > len(self._samples):\n",
        "            return random.sample(self._samples, len(self._samples))\n",
        "        else:\n",
        "            return random.sample(self._samples, no_samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "nT284hcB2toW",
        "outputId": "cd9de226-b507-4306-d904-7732e69bf485"
      },
      "source": [
        "class GameRunner:\n",
        "    def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
        "                 decay, render=True):\n",
        "        self._sess = sess\n",
        "        self._env = env\n",
        "        self._model = model\n",
        "        self._memory = memory\n",
        "        self._render = render\n",
        "        self._max_eps = max_eps\n",
        "        self._min_eps = min_eps\n",
        "        self._decay = decay\n",
        "        self._eps = self._max_eps\n",
        "        self._steps = 0\n",
        "        self._reward_store = []\n",
        "        self._max_x_store = []\n",
        "\n",
        "    def run(self):\n",
        "        state = self._env.reset()\n",
        "        tot_reward = 0\n",
        "        max_x = -100\n",
        "        while True:\n",
        "            if self._render:\n",
        "                self._env.render()\n",
        "\n",
        "            action = self._choose_action(state)\n",
        "            next_state, reward, done, info = self._env.step(action)\n",
        "            if next_state[0] >= 0.1:\n",
        "                reward += 10\n",
        "            elif next_state[0] >= 0.25:\n",
        "                reward += 20\n",
        "            elif next_state[0] >= 0.5:\n",
        "                reward += 100\n",
        "\n",
        "            if next_state[0] > max_x:\n",
        "                max_x = next_state[0]\n",
        "            # is the game complete? If so, set the next state to\n",
        "            # None for storage sake\n",
        "            if done:\n",
        "                next_state = None\n",
        "\n",
        "            self._memory.add_sample((state, action, reward, next_state))\n",
        "            self._replay()\n",
        "\n",
        "            # exponentially decay the eps value\n",
        "            self._steps += 1\n",
        "            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \\\n",
        "                                      * math.exp(-LAMBDA * self._steps)\n",
        "\n",
        "            # move the agent to the next state and accumulate the reward\n",
        "            state = next_state\n",
        "            tot_reward += reward\n",
        "\n",
        "            # if the game is done, break the loop\n",
        "            if done:\n",
        "                self._reward_store.append(tot_reward)\n",
        "                self._max_x_store.append(max_x)\n",
        "                break\n",
        "\n",
        "        print(\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
        "\n",
        "    def _choose_action(self, state):\n",
        "        if random.random() < self._eps:\n",
        "            return random.randint(0, self._model.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(self._model.predict_one(state, self._sess))\n",
        "\n",
        "    def _replay(self):\n",
        "        batch = self._memory.sample(self._model.batch_size)\n",
        "        states = np.array([val[0] for val in batch])\n",
        "        next_states = np.array([(np.zeros(self._model.num_states)\n",
        "                                 if val[3] is None else val[3]) for val in batch])\n",
        "        # predict Q(s,a) given the batch of states\n",
        "        q_s_a = self._model.predict_batch(states, self._sess)\n",
        "        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
        "        q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
        "        # setup training arrays\n",
        "        x = np.zeros((len(batch), self._model.num_states))\n",
        "        y = np.zeros((len(batch), self._model.num_actions))\n",
        "        for i, b in enumerate(batch):\n",
        "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
        "            # get the current q values for all actions in state\n",
        "            current_q = q_s_a[i]\n",
        "            # update the q value for action\n",
        "            if next_state is None:\n",
        "                # in this case, the game completed after action, so there is no max Q(s',a')\n",
        "                # prediction possible\n",
        "                current_q[action] = reward\n",
        "            else:\n",
        "                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
        "            x[i] = state\n",
        "            y[i] = current_q\n",
        "        self._model.train_batch(self._sess, x, y)\n",
        "\n",
        "    @property\n",
        "    def reward_store(self):\n",
        "        return self._reward_store\n",
        "\n",
        "    @property\n",
        "    def max_x_store(self):\n",
        "        return self._max_x_store\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_name = 'MountainCar-v0'\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    num_states = env.env.observation_space.shape[0]\n",
        "    num_actions = env.env.action_space.n\n",
        "\n",
        "    model = Model(num_states, num_actions, BATCH_SIZE)\n",
        "    mem = Memory(50000)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.var_init)\n",
        "        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,\n",
        "                        LAMBDA)\n",
        "        num_episodes = 300\n",
        "        cnt = 0\n",
        "        while cnt < num_episodes:\n",
        "            if cnt % 10 == 0:\n",
        "                print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
        "            gr.run()\n",
        "            cnt += 1\n",
        "        plt.plot(gr.reward_store)\n",
        "        plt.show()\n",
        "        plt.close(\"all\")\n",
        "        plt.plot(gr.max_x_store)\n",
        "        plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-5320e6ad77e2>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From C:\\Users\\dhupee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F46920F348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F46920F348>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F46920F348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F46920F348>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F468EAD048>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:From C:\\Users\\dhupee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Episode 1 of 300\n",
            "Step 200, Total reward: -200.0, Eps: 0.9803966865736877\n",
            "Step 400, Total reward: -200.0, Eps: 0.9611815447608\n",
            "Step 600, Total reward: -200.0, Eps: 0.9423468882484062\n",
            "Step 800, Total reward: -200.0, Eps: 0.9238851829227694\n",
            "Step 1000, Total reward: -200.0, Eps: 0.9057890438555999\n",
            "Step 1200, Total reward: -200.0, Eps: 0.888051232349986\n",
            "Step 1400, Total reward: -200.0, Eps: 0.8706646530448178\n",
            "Step 1600, Total reward: -200.0, Eps: 0.8536223510765493\n",
            "Step 1800, Total reward: -200.0, Eps: 0.8369175092971592\n",
            "Step 2000, Total reward: -200.0, Eps: 0.820543445547202\n",
            "Episode 11 of 300\n",
            "Step 2200, Total reward: -200.0, Eps: 0.8044936099828537\n",
            "Step 2400, Total reward: -200.0, Eps: 0.7887615824558878\n",
            "Step 2600, Total reward: -200.0, Eps: 0.7733410699455306\n",
            "Step 2800, Total reward: -200.0, Eps: 0.7582259040411682\n",
            "Step 3000, Total reward: -200.0, Eps: 0.7434100384749007\n",
            "Step 3200, Total reward: -200.0, Eps: 0.7288875467029541\n",
            "Step 3400, Total reward: -200.0, Eps: 0.7146526195349836\n",
            "Step 3600, Total reward: -200.0, Eps: 0.7006995628103208\n",
            "Step 3800, Total reward: -200.0, Eps: 0.6870227951202322\n",
            "Step 4000, Total reward: -200.0, Eps: 0.6736168455752829\n",
            "Episode 21 of 300\n",
            "Step 4200, Total reward: -200.0, Eps: 0.6604763516169062\n",
            "Step 4400, Total reward: -200.0, Eps: 0.64759605687231\n",
            "Step 4600, Total reward: -200.0, Eps: 0.6349708090518567\n",
            "Step 4800, Total reward: -200.0, Eps: 0.6225955578880794\n",
            "Step 5000, Total reward: -200.0, Eps: 0.6104653531155071\n",
            "Step 5200, Total reward: -200.0, Eps: 0.5985753424904925\n",
            "Step 5400, Total reward: -200.0, Eps: 0.5869207698502498\n",
            "Step 5600, Total reward: -200.0, Eps: 0.5754969732103268\n",
            "Step 5800, Total reward: -200.0, Eps: 0.564299382899748\n",
            "Step 6000, Total reward: -200.0, Eps: 0.5533235197330862\n",
            "Episode 31 of 300\n",
            "Step 6200, Total reward: -200.0, Eps: 0.5425649932187278\n",
            "Step 6400, Total reward: -200.0, Eps: 0.5320194998026181\n",
            "Step 6600, Total reward: -200.0, Eps: 0.5216828211467822\n",
            "Step 6800, Total reward: -200.0, Eps: 0.5115508224419336\n",
            "Step 7000, Total reward: -200.0, Eps: 0.5016194507534953\n",
            "Step 7200, Total reward: -200.0, Eps: 0.4918847334003719\n",
            "Step 7400, Total reward: -200.0, Eps: 0.48234277636582407\n",
            "Step 7600, Total reward: -200.0, Eps: 0.47298976273981014\n",
            "Step 7800, Total reward: -200.0, Eps: 0.4638219511921713\n",
            "Step 8000, Total reward: -200.0, Eps: 0.45483567447604933\n",
            "Episode 41 of 300\n",
            "Step 8200, Total reward: -200.0, Eps: 0.44602733796093924\n",
            "Step 8400, Total reward: -200.0, Eps: 0.4373934181947889\n",
            "Step 8600, Total reward: -200.0, Eps: 0.4289304614945713\n",
            "Step 8800, Total reward: -200.0, Eps: 0.42063508256476556\n",
            "Step 9000, Total reward: -200.0, Eps: 0.4125039631431931\n",
            "Step 9200, Total reward: -200.0, Eps: 0.404533850673669\n",
            "Step 9400, Total reward: -200.0, Eps: 0.3967215570049359\n",
            "Step 9600, Total reward: -200.0, Eps: 0.3890639571153609\n",
            "Step 9800, Total reward: -200.0, Eps: 0.38155798786288553\n",
            "Step 10000, Total reward: -200.0, Eps: 0.3742006467597279\n",
            "Episode 51 of 300\n",
            "Step 10200, Total reward: -200.0, Eps: 0.3669889907713475\n",
            "Step 10400, Total reward: -200.0, Eps: 0.35992013513919235\n",
            "Step 10600, Total reward: -200.0, Eps: 0.3529912522267568\n",
            "Step 10800, Total reward: -200.0, Eps: 0.34619957038848975\n",
            "Step 11000, Total reward: -200.0, Eps: 0.3395423728610988\n",
            "Step 11200, Total reward: -200.0, Eps: 0.33301699667680906\n",
            "Step 11400, Total reward: -200.0, Eps: 0.3266208315981408\n",
            "Step 11600, Total reward: -200.0, Eps: 0.32035131907377923\n",
            "Step 11800, Total reward: -200.0, Eps: 0.3142059512151199\n",
            "Step 12000, Total reward: -200.0, Eps: 0.3081822697930801\n",
            "Episode 61 of 300\n",
            "Step 12200, Total reward: -200.0, Eps: 0.30227786525477407\n",
            "Step 12400, Total reward: -200.0, Eps: 0.29649037575966014\n",
            "Step 12600, Total reward: -200.0, Eps: 0.2908174862347727\n",
            "Step 12800, Total reward: -200.0, Eps: 0.2852569274486622\n",
            "Step 13000, Total reward: -200.0, Eps: 0.27980647510367246\n",
            "Step 13200, Total reward: -200.0, Eps: 0.27446394894619186\n",
            "Step 13400, Total reward: -200.0, Eps: 0.26922721189452276\n",
            "Step 13600, Total reward: -200.0, Eps: 0.26409416918402034\n",
            "Step 13800, Total reward: -200.0, Eps: 0.2590627675291589\n",
            "Step 14000, Total reward: -140.0, Eps: 0.2541309943021904\n",
            "Episode 71 of 300\n",
            "Step 14200, Total reward: -200.0, Eps: 0.24929687672806605\n",
            "Step 14400, Total reward: -200.0, Eps: 0.2445584810953005\n",
            "Step 14600, Total reward: -200.0, Eps: 0.23991391198246126\n",
            "Step 14800, Total reward: -200.0, Eps: 0.2353613114999746\n",
            "Step 15000, Total reward: -200.0, Eps: 0.23089885854694553\n",
            "Step 15200, Total reward: -200.0, Eps: 0.22652476808269262\n",
            "Step 15400, Total reward: -200.0, Eps: 0.22223729041270818\n",
            "Step 15600, Total reward: -200.0, Eps: 0.21803471048875708\n",
            "Step 15800, Total reward: -200.0, Eps: 0.21391534722283462\n",
            "Step 16000, Total reward: -200.0, Eps: 0.20987755281470882\n",
            "Episode 81 of 300\n",
            "Step 16200, Total reward: -200.0, Eps: 0.2059197120927785\n",
            "Step 16400, Total reward: -200.0, Eps: 0.20204024186798297\n",
            "Step 16600, Total reward: -200.0, Eps: 0.1982375903005053\n",
            "Step 16800, Total reward: -200.0, Eps: 0.19451023627901584\n",
            "Step 17000, Total reward: -200.0, Eps: 0.19085668881220727\n",
            "Step 17200, Total reward: -110.0, Eps: 0.1872754864323783\n",
            "Step 17400, Total reward: -110.0, Eps: 0.1837651966108269\n",
            "Step 17600, Total reward: -200.0, Eps: 0.18032441518482004\n",
            "Step 17791, Total reward: -81.0, Eps: 0.1771020900208802\n",
            "Step 17991, Total reward: -200.0, Eps: 0.17379324694525278\n",
            "Episode 91 of 300\n",
            "Step 18155, Total reward: 46.0, Eps: 0.1711289446894749\n",
            "Step 18355, Total reward: 0.0, Eps: 0.16793837781594084\n",
            "Step 18555, Total reward: -200.0, Eps: 0.16481098839940628\n",
            "Step 18755, Total reward: -200.0, Eps: 0.16174552544240553\n",
            "Step 18955, Total reward: -200.0, Eps: 0.15874076271888235\n",
            "Step 19155, Total reward: -200.0, Eps: 0.1557954982836834\n",
            "Step 19355, Total reward: -200.0, Eps: 0.1529085539917638\n",
            "Step 19548, Total reward: 57.0, Eps: 0.1501768644967416\n",
            "Step 19748, Total reward: -200.0, Eps: 0.14740117660800697\n",
            "Step 19948, Total reward: 130.0, Eps: 0.14468045102195562\n",
            "Episode 101 of 300\n",
            "Step 20148, Total reward: -200.0, Eps: 0.14201359941207634\n",
            "Step 20348, Total reward: -110.0, Eps: 0.13939955500216664\n",
            "Step 20548, Total reward: -200.0, Eps: 0.13683727213960828\n",
            "Step 20748, Total reward: -200.0, Eps: 0.13432572587709188\n",
            "Step 20948, Total reward: -200.0, Eps: 0.1318639115626248\n",
            "Step 21148, Total reward: -200.0, Eps: 0.1294508444376566\n",
            "Step 21348, Total reward: -200.0, Eps: 0.1270855592431626\n",
            "Step 21548, Total reward: -200.0, Eps: 0.12476710983352747\n",
            "Step 21748, Total reward: -200.0, Eps: 0.12249456879807429\n",
            "Step 21941, Total reward: -43.0, Eps: 0.12034424103077734\n",
            "Episode 111 of 300\n",
            "Step 22083, Total reward: -2.0, Eps: 0.11878842524303672\n",
            "Step 22283, Total reward: 180.0, Eps: 0.11663427009435572\n",
            "Step 22483, Total reward: -200.0, Eps: 0.1145227700755217\n",
            "Step 22683, Total reward: -200.0, Eps: 0.11245308055837337\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-2e24502af853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode {} of {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_store\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-8-2e24502af853>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_choose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-8-2e24502af853>\u001b[0m in \u001b[0;36m_choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actions\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-5320e6ad77e2>\u001b[0m in \u001b[0;36mpredict_one\u001b[1;34m(self, state, sess)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         return sess.run(self._logits, feed_dict={self._states:\n\u001b[1;32m---> 29\u001b[1;33m                                                      state.reshape(1, self.num_states)})\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}